{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Introduction:\n",
    "\n",
    "This project is intended to be an initial exploration and creation of a potential classification machine learning algorithm that can be used and integrated into the Olin ABE(Amorphous Blob of Events), a platform that enables tthe creation of digitial experiences that share information about past, present, and upcoming events. \n",
    "\n",
    "Today email is an essential part of how Olin communicates and conveys information, there exists lists such as carpediem which are used solely to convey information about things that are happening. Ideally this information could be automatically captured by ABE amd turned into an event; however, the format of carpediem emails is non standard. Thus, a system must be designed and created to extract, load, and create an appropriate event for the email depending on the information inside. One aspect of this system would be a method to create an appropriate tag or tags for the project based on the content of the email. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data exploration sought to find patterns about the subject and body text data that is provided to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried two main algorithms: Bag of Words with SciKit Learn and Naive Bayesian by Hand\n",
    "\n",
    "Bag of words consists of transforming each data point (sentence) into a sparse vector that corresponds a consistent row of word - index relationship. This can be easily done using scikit-learns Count Vectorizer. However the preprocessing of the data can result in different results. We tried to use our insights from the data exploration to in addition to removing a list of stopwords from online to improve the choosing the most influential word. However, because we our exploration lacked enough data to really extract useful features and insights I stuck with the original model and vectorizing tool, which is able to refer to outside langauge data.\n",
    "\n",
    "We first followed the bag of words kaggle tutorial (https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words) to create a (256060, 1000) bag of words as a feature map, and applied random forest classification to fit our trainning data. Due to limitedness of memory, we were only able to perform random forest classification at tree level of 10.\n",
    "\n",
    "Naive Bayesian is a simple but powerful probabilistic classifier that relies on applying Baye’s theorem to data by assuming that the features (in a bag of words case each word feature) are strongly independent. I specifically chose to do our own implementation because of the simplistic nature but also common application of this technique in sentiment analysis. To learn about the math and its applications to sentiment analysis I followed http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/ as a guide and then created our own implementation of the mathematical equations for Multinomial Bayesian Classifier, Multinomial Bayesian Classifier with smoothing, and Binarized Multinomial Bayesian Classifier with smoothing.\n",
    "\n",
    "\n",
    "\n",
    "Math and Code Explanation:\n",
    "\n",
    "To apply naive bayesian to our this data set we have 28 separate outcomes (tags) that we will need to calculate the probability of for each email we are provided and need a set of 28 class probabilities and a reference set of 28 outcome probabilities for each word.\n",
    "\n",
    "We can get the class probabilities fairly easily by counting the number of phrases with that sentiment and dividing by the total number of phrases\n",
    "\n",
    "We will can get the frequency per word for each probability and then calculate the probability for each of the class probabilities by dividing the the total number of occurrences that word occurs in any given sentiment outcome by the frequency of word\n",
    "\n",
    "Then, to get the probability of a phrase or sentence we multiply the probability for each word in that phrase against the probability of any document that expresses that tag.\n",
    "\n",
    "Finally the model can be improved by adding smoothing factors to the calculation of the word probabilities.\n",
    "\n",
    "Multinomial Naive Bayesian\n",
    "Reference-style: \n",
    "![alt text][logo]\n",
    "\n",
    "[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\"\n",
    "Multinomial Naive Bayesian w/ Smoothing\n",
    "\n",
    "Reference-style: \n",
    "![alt text][logo]\n",
    "\n",
    "[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\"\n",
    "\n",
    "Lessons Learned from implementing different Bayesian classifiers\n",
    "\n",
    "Smoothing makes a big difference. a. We specifically implemented smoothing by adding 1 as part of additive smoothing b. We made the impact of any word that didn’t exist irrelevant c. We added the total # of the frequency of that word as a smoothing factor\n",
    "Binarized was an expected and actual improvement because a. We only care about whether the word occurs in the sentence not how many times b. By only counting a word once we reduce the impact and risk of overfitting\n",
    "We started off by doing a multinomial naive bayesian model without any smoothing. Adding smoothing improved the score significantly and then we only counted the frequency of words inside a particular phrase bc we care about whether the word exist rather than how many times it occurs (Binarized multinomial). Our final combination of Binarized multinomial bayes with smoothing was the best outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the classification algorithm: the code to load datasets is ready. The current data + model for the sake of scope is only designed to choose one tag when many events have multiple tags (for example, many events often can include a food tag). \n",
    "\n",
    "Possible extensions are \n",
    "1. Gathering more data and rerunning the data exploration code to see if there are more noticeble trends that can be useful to us later.\n",
    "2. Extending the data exploration to exploring the temporal data provided in date and time.\n",
    "3. Extending into other multiclassification algorithms such as neural networks as well as applying the naive bayesian work already done.\n",
    "\n",
    "There may be additional visualizations that may be useful to extract additional features to assist in classificiation. The current _ success rate is also limited by the amount of data (currently N = )\n",
    "\n",
    "Working through the data processing, visualization, and training processes has given lot's of insight into what would be necessary to automatically create events from an email. \n",
    "\n",
    "The biggest risks are in the \n",
    "1. High variability of emails that include non event items such as\n",
    "    a. things for free \n",
    "    b. things for sale\n",
    "    c. housing offers\n",
    "2. Variable inclusion and formating of temporal and location data\n",
    "3. Dealing with Re:'s \n",
    "\n",
    "\n",
    "Given the constraints I propose a potential system consisting of the following pieces:\n",
    "\n",
    "1. Email Scrapper through Pop (can use the Data Scrapping and Processing Code) \n",
    "2. \n",
    "3. Database \n",
    "4. App with classification algorithm "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
