{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial attempt to classify using 3 seperate algorithms:\n",
    "1. Bag of words ((https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words) with random forest classification to fit our training data. Due to limitedness of memory, we were only able to perform random forest classification at tree level of 10. \n",
    "2. Naive Bayesian, a simple but powerful probabilistic classifier that relies on applying Bayeâ€™s theorem to data by assuming that the features are strongly independent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wilsontang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def text_to_words(text):\n",
    "    \"\"\"This function will take in a loaded text as well as a list of stop words and process it by\n",
    "    removing non letters, converting to lowercase and splitting, removing stopwords, and \n",
    "    returning a reconstructed text\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return ( \" \".join(meaningful_words ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a bag of words, we need to do some level of data cleaning beforehand. It would be sad if some of the common meaningless stop words like \"the\", \"a\" becomes the features of our bag of word. After that, we will use scikit-learn's bag of words tool \"CountVectorizer\" to generate a vectorizer of 1000 words. By using the bag of words, we then use random forest classification of level 10 to fit our training bag of words. After following the tutorial, we want to incorporate our discoverings from data exploration to the bag of words. Specifically, we wanted to filter out words of high standard deviation while keeping those with high frequency. We first import both train and test data and drop \"Nan\" entries for basic data cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             who                                   date  \\\n",
      "0                  Rowan Sharman      Thursday, May 10, 2018 2:24:37 AM   \n",
      "1                 Nina Tchirkova      Wednesday, May 9, 2018 9:05:55 PM   \n",
      "2                    Logan Sweet      Wednesday, May 9, 2018 7:55:25 PM   \n",
      "3                    Miranda Lao      Wednesday, May 9, 2018 7:24:50 PM   \n",
      "4                    Lucy Wilcox      Wednesday, May 9, 2018 7:00:51 PM   \n",
      "5                     Cedric Kim      Wednesday, May 9, 2018 5:09:13 PM   \n",
      "6                    Paul Ruvolo      Wednesday, May 9, 2018 4:40:50 PM   \n",
      "7                Matthew Brucker     Wednesday, May 9, 2018 11:46:40 AM   \n",
      "8                    Eric Miller     Wednesday, May 9, 2018 11:45:34 AM   \n",
      "9                  Maya Calabria     Wednesday, May 9, 2018 10:25:37 AM   \n",
      "10                   Logan Sweet       Tuesday, May 8, 2018 10:28:52 PM   \n",
      "11                 Rowan Sharman        Tuesday, May 8, 2018 6:14:31 PM   \n",
      "12             Samantha Eppinger        Tuesday, May 8, 2018 3:49:52 PM   \n",
      "13                  Ariana Olson        Tuesday, May 8, 2018 3:26:28 PM   \n",
      "14                  Emily Kohler        Tuesday, May 8, 2018 2:20:31 PM   \n",
      "15                  Amon Millner       Tuesday, May 8, 2018 12:27:19 PM   \n",
      "16                Sophia Nielsen        Monday, May 7, 2018 10:02:10 PM   \n",
      "17                David Abrahams         Monday, May 7, 2018 8:40:08 PM   \n",
      "18                  Emily Nasiff         Monday, May 7, 2018 7:54:13 PM   \n",
      "19                Missoury Lytle         Monday, May 7, 2018 6:09:59 PM   \n",
      "21                 Kevin Crispie         Monday, May 7, 2018 4:24:35 PM   \n",
      "22                    William Lu         Monday, May 7, 2018 4:01:15 PM   \n",
      "23              Miranda McMillen         Monday, May 7, 2018 2:29:47 PM   \n",
      "24                   Evan Cusato        Monday, May 7, 2018 12:29:17 PM   \n",
      "25                   Shruti Iyer        Monday, May 7, 2018 12:24:24 PM   \n",
      "26                     Erica Lee         Sunday, May 6, 2018 8:03:36 PM   \n",
      "27                Tehya Stockman         Sunday, May 6, 2018 5:39:48 PM   \n",
      "28                     Arpan Rau         Sunday, May 6, 2018 5:32:42 PM   \n",
      "29               Mikhaela Dietch         Sunday, May 6, 2018 4:29:28 PM   \n",
      "30                 Rowan Sharman         Sunday, May 6, 2018 4:21:42 PM   \n",
      "..                           ...                                    ...   \n",
      "149  Colvin Chapman (Forwarding)     Monday, April 16, 2018 12:09:52 PM   \n",
      "150               Nina Tchirkova      Sunday, April 15, 2018 8:08:26 PM   \n",
      "151                  Chloe Grubb      Sunday, April 15, 2018 7:44:18 PM   \n",
      "152                  Chloe Grubb      Sunday, April 15, 2018 5:54:51 PM   \n",
      "153                 Isaac Vandor      Sunday, April 15, 2018 3:40:51 PM   \n",
      "156                Diego Alvarez     Sunday, April 15, 2018 12:57:39 PM   \n",
      "157                Keenan Zucker     Sunday, April 15, 2018 10:52:51 AM   \n",
      "158            Lakhvinder Jordan   Saturday, April 14, 2018 10:11:42 PM   \n",
      "159               Nina Tchirkova     Sunday, April 15, 2018 12:53:55 PM   \n",
      "160              Katerina Soltan    Saturday, April 14, 2018 9:05:10 AM   \n",
      "161                Diego Alvarez      Friday, April 13, 2018 9:05:08 PM   \n",
      "162              Taylor Sheneman      Friday, April 13, 2018 9:01:03 PM   \n",
      "163                  Erika Serna      Friday, April 13, 2018 6:46:00 PM   \n",
      "165                Antonio Perez  Wednesday, April 11, 2018 11:09:08 PM   \n",
      "166                  Erika Serna      Friday, April 13, 2018 2:05:23 AM   \n",
      "167                  Chloe Grubb   Thursday, April 12, 2018 10:06:13 AM   \n",
      "168                  Abigail Fry    Thursday, April 12, 2018 8:15:48 PM   \n",
      "169             Raymundo Camacho    Thursday, April 12, 2018 6:38:45 PM   \n",
      "170               Nina Tchirkova    Thursday, April 12, 2018 7:39:33 PM   \n",
      "171              Alexander Hoppe    Thursday, April 12, 2018 9:33:15 AM   \n",
      "172            Lakhvinder Jordan   Thursday, April 12, 2018 12:33:59 PM   \n",
      "173                David Freeman   Thursday, April 12, 2018 11:33:44 AM   \n",
      "174                     Kai Levy   Thursday, April 12, 2018 10:46:21 AM   \n",
      "175                 Emily Kohler   Wednesday, April 11, 2018 8:08:19 PM   \n",
      "176         Mackenzie Frackleton   Wednesday, April 11, 2018 8:01:36 PM   \n",
      "177                 Samuel Myers   Wednesday, April 11, 2018 5:17:58 PM   \n",
      "178                Maxmilian Wei   Wednesday, April 11, 2018 3:32:24 PM   \n",
      "180               Alyson Goodrow   Wednesday, April 11, 2018 1:31:08 PM   \n",
      "181            Lakhvinder Jordan    Tuesday, April 10, 2018 12:38:33 PM   \n",
      "182                 Lydia Hodges      Friday, April 20, 2018 8:15:23 PM   \n",
      "\n",
      "                                               subject  \\\n",
      "0                    [Carpediem] Free wh shelf top box   \n",
      "1    [Carpediem] Byron and Keenan are talking in th...   \n",
      "2      [Carpediem] Bedroom for lease in Central Square   \n",
      "3                          [Carpediem] Fidget Spinners   \n",
      "4                                     [Carpediem] Cake   \n",
      "5                          [Carpediem] Selling 18650's   \n",
      "6    [Carpediem] FW: Design with & for blind popula...   \n",
      "7                                 [Carpediem] Free PVC   \n",
      "8    [Carpediem] [Resolved] Care for my car this su...   \n",
      "9                         [Carpediem] NO ACRONYM TODAY   \n",
      "10              Re: [Carpediem] Seniors Selling Stuff!   \n",
      "11                              [Carpediem] FREE STUFF   \n",
      "12   [Carpediem] ADE Final Presentation and Poster ...   \n",
      "13         [Carpediem] Babysit my couch for the summer   \n",
      "14                               [Carpediem] 2 couches   \n",
      "15   [Carpediem] The Software Design Final Open Hou...   \n",
      "16           [Carpediem] Stuff to borrow, Stuff to buy   \n",
      "17   carpediem@lists.olin.edu; pingpongopen2018@lis...   \n",
      "18                   [Carpediem] Jumanji in the Nord!!   \n",
      "19                [Carpediem] Come to ADE Final Event!   \n",
      "21   [Carpediem] Quality Suite/Dorm Room Tables for...   \n",
      "22   [Carpediem] Fwd: [PoolOpen2018] Grand Finals Date   \n",
      "23                     [Carpediem] Couch Sitter Wanted   \n",
      "24              [Carpediem] FREE Wood WH North Storage   \n",
      "25         Re: [Carpediem] Selling AJR Concert Tickets   \n",
      "26                    [Carpediem] Futon for the Summer   \n",
      "27                  [Carpediem] Freezer for the Summer   \n",
      "28                          Re: [Carpediem] Free futon   \n",
      "29   [Carpediem] Selling Klipsch ProMedia Speakers ...   \n",
      "30                                   [Carpediem] Winch   \n",
      "..                                                 ...   \n",
      "149                   [Carpediem] Tie dye rinse party!   \n",
      "150              [Carpediem] Jazz 8:30 in the library!   \n",
      "151      [Carpediem] SLACfest leftovers in EH kitchen!   \n",
      "152      [Carpediem] Locally/sustainably sourced meal!   \n",
      "153  [Carpediem] HackingOlin Town Hall Meeting Idea...   \n",
      "156           [Carpediem] Final Backburner Menu Survey   \n",
      "157  [Carpediem] SLACfest is happening! Pancake art...   \n",
      "158                      [Carpediem] 4E Party time EOM   \n",
      "159                    [Carpediem] DIY Grilled Cheese!   \n",
      "160                             [Carpediem] Slackline!   \n",
      "161                   [Carpediem] Backburner Now Open!   \n",
      "162  [Carpediem] Fireside: closed for repairs (stil...   \n",
      "163                  [Carpediem] PEANUT BUTTER COOKIES   \n",
      "165  [Carpediem] English Second Language Teaching O...   \n",
      "166                   [Carpediem] OPEN's Spring Event!   \n",
      "167                          [Carpediem] Cool HRI Talk   \n",
      "168       [Carpediem] Free snacks in West Hall Kitchen   \n",
      "169       [Carpediem] Murder in the Dark Friday the 13   \n",
      "170         [Carpediem] Do you like cooking or baking?   \n",
      "171                           [Carpediem] Thursd Wave!   \n",
      "172   [Carpediem] Come talk about the SG Constitution!   \n",
      "173                 [Carpediem] U.S. Girls FREE TICKET   \n",
      "174                 griffin.tschurwald@alumni.olin.edu   \n",
      "175                    [Carpediem] Contraception Panel   \n",
      "176         [Carpediem] Pizza, soda, cake, and Amazon!   \n",
      "177       [Carpediem] Free Sox Ticket - like right now   \n",
      "178                             [Carpediem] H mart run   \n",
      "180                                     Jonathan Adler   \n",
      "181  [Carpediem] Come talk about the SG Constitutio...   \n",
      "182            [Carpediem] STICKERS! Non-Monetary Cost   \n",
      "\n",
      "                                                  body  Tag  \\\n",
      "0    Increases the surface area of the top of your ...   26   \n",
      "1    EOM __________________________________________...   15   \n",
      "2    A couple friends of mine are looking for a thi...   27   \n",
      "3    I have many and would like to have fewer. they...   26   \n",
      "4    Cake and cookies in the hallway entrance to th...    2   \n",
      "5    I have about 200 18650 cells (cylindrical li-i...   26   \n",
      "6    If you=E2=80=99re looking for something to do ...   11   \n",
      "7    Hi all, I have a bunch of pieces of 1=E2=80=9D...   26   \n",
      "8    Resolved. Thanks! On Wed, May 9, 2018 at 11:31...   28   \n",
      "9    =E2=80=8BNo ACRONYM today, but we'll be doing ...    2   \n",
      "10   =E2=80=8Bbumping this because a bunch has been...   26   \n",
      "11   TOOLS, STOCK, HARDWARE, BIKE PARTS, T-SHIRTS, ...   26   \n",
      "12   Come to the ADE final presentations and poster...   11   \n",
      "13   Looking for a temporary home for my couch. If ...   28   \n",
      "14   I have 2 couches that I no longer need. If you...   26   \n",
      "15   Please join the Spring 2018 Software Design Fi...   11   \n",
      "16   tldr; want stuff? Go here <https://docs.google...   26   \n",
      "17   Currently Shane is up 3-2 in games. Games are ...    6   \n",
      "18   Study break brought to you by SAC. Jumanji in ...   19   \n",
      "19   ADE Social Ventures Spring Expo: TUES, MAY 8, ...   11   \n",
      "21   Hi everyone, Sunny, Kevin and I made some awes...   26   \n",
      "22                                      Happening now!   18   \n",
      "23   I have a very comfy couch that needs a sitter ...   28   \n",
      "24   We have a great assortment of 2x4s, 2x6s, 4x4s...   26   \n",
      "25   One ticket left! From: Carpediem <carpediem-bo...   26   \n",
      "26   I am currently in possession of a futon that n...   28   \n",
      "27   Does anyone staying at Olin this summer want a...   28   \n",
      "28   Resolved! On Sun, May 6, 2018, 3:56 PM Arpan R...   26   \n",
      "29   Hey Ya=E2=80=99ll I=E2=80=99ve priced dropped ...   26   \n",
      "30   New AC winch with pendant control. 19ft at 440...   26   \n",
      "..                                                 ...  ...   \n",
      "149  If you made a shirt at slacfest, come get it a...   18   \n",
      "150  There is also bread pudding! _________________...   19   \n",
      "151  Hello again, Thank you to everyone who came an...    2   \n",
      "152  Hey friends! As you may know, this years SLACf...    2   \n",
      "153  Hey carpe, HackingOlin will be leading an idea...   24   \n",
      "156  The final opening of backburner approaches=E2=...    2   \n",
      "157  Come to the library ALL DAY for endless food a...   15   \n",
      "158  ______________________________________________...   18   \n",
      "159  Library now! There are avocados ______________...    2   \n",
      "160  Hey all! I put up a slackline in front of WH. ...   28   \n",
      "161  Backburner is now open in WH407! As a reminder...    2   \n",
      "162  Also I think I made myself sick working with p...   28   \n",
      "163  HEY, PEANUT BUTTER COOKIES IN THE LOWER LEVEL ...    2   \n",
      "165  Hello all! This past year, Olin students and f...    1   \n",
      "166  Good evening everyone, I'm gay. Also there's a...   10   \n",
      "167  Hello, A good friend of mine, Ross Mead, is in...   24   \n",
      "168  -Baklava on the table -Fruit in the fridge -Hu...    2   \n",
      "169  Hello everyone, Tomorrow is Friday the 13 and ...   18   \n",
      "170  SLACfest needs your help for Sunday evening's ...   15   \n",
      "171  Hey friends! After reading a book on running c...    2   \n",
      "172  TOWN HALL MEETING TOMORROW 4:30PM IN THE NORD!...   24   \n",
      "173  U.S. Girls are playing at the Great Scott in A...   26   \n",
      "174  A few Olin alums have a spot available in thei...   28   \n",
      "175  Hello! I=E2=80=99m planning a panel for people...   24   \n",
      "176  Happening now in the library! Cone talk to coo...    2   \n",
      "177  Want to come watch the Red Sox beat up the Yan...   19   \n",
      "178  Going on an hmart run for backburner either la...   25   \n",
      "180  This Jansport backpack has been sitting in the...   26   \n",
      "181  Hey everyone! REMINDER: Please come talk about...   24   \n",
      "182  Hey Carpe, Another P&M project, another survey...   26   \n",
      "\n",
      "                                        processed_body  \n",
      "0    increases surface area top shelf sticks front ...  \n",
      "1    eom carpediem mailing list carpediem lists oli...  \n",
      "2    couple friends mine looking third roommate hou...  \n",
      "3    many would like fewer brand new literally stil...  \n",
      "4    cake cookies hallway entrance auditorium left ...  \n",
      "5    cells cylindrical li ion cells old laptops pla...  \n",
      "6    e looking something fine evening consider wing...  \n",
      "7    hi bunch pieces e pvc pipe e need range length...  \n",
      "8    resolved thanks wed may eric miller eric mille...  \n",
      "9    e bno acronym today special end year eve nt la...  \n",
      "10                              e bbumping bunch added  \n",
      "11   tools stock hardware bike parts shirts random ...  \n",
      "12   come ade final presentations poster session ro...  \n",
      "13   looking temporary home couch staying campus su...  \n",
      "14   couches longer need interested one please mess...  \n",
      "15   please join spring software design final open ...  \n",
      "16   tldr want stuff go https docs google com sprea...  \n",
      "17   currently shane games games first games wins l...  \n",
      "18                study break brought sac jumanji nord  \n",
      "19   ade social ventures spring expo tues may pm ol...  \n",
      "21   hi everyone sunny kevin made awesome tables su...  \n",
      "22                                           happening  \n",
      "23   comfy couch needs sitter summer also shelving ...  \n",
      "24   great assortment x x x custom table could use ...  \n",
      "25   one ticket left carpediem carpediem bounces li...  \n",
      "26   currently possession futon needs home summer g...  \n",
      "27   anyone staying olin summer want extra freezer ...  \n",
      "28   resolved sun may pm arpan rau arpan rau studen...  \n",
      "29   hey ya e e priced dropped klipsch promedia thx...  \n",
      "30   new ac winch pendant control ft lbs ft lbs che...  \n",
      "..                                                 ...  \n",
      "149  made shirt slacfest come get wash else e proba...  \n",
      "150  also bread pudding carpediem mailing list carp...  \n",
      "151  hello thank everyone came ate slacfest communa...  \n",
      "152  hey friends may know years slacfest meal local...  \n",
      "153  hey carpe hackingolin leading ideation session...  \n",
      "156  final opening backburner approaches e want kno...  \n",
      "157  come library day endless food fun wow e ex cit...  \n",
      "158  carpediem mailing list carpediem lists olin ed...  \n",
      "159  library avocados carpediem mailing list carped...  \n",
      "160  hey put slackline front wh feel free use let k...  \n",
      "161  backburner open wh reminder tonight e options ...  \n",
      "162  also think made sick working pvc cement closed...  \n",
      "163  hey peanut butter cookies lower level library ...  \n",
      "165  hello past year olin students faculty collabor...  \n",
      "166  good evening everyone gay also couple upcoming...  \n",
      "167  hello good friend mine ross mead town hanging ...  \n",
      "168  baklava table fruit fridge hummus dips veggies...  \n",
      "169  hello everyone tomorrow friday better day murd...  \n",
      "170  slacfest needs help sunday evening community f...  \n",
      "171  hey friends reading book running coffee shops ...  \n",
      "172  town hall meeting tomorrow pm nord need quorum...  \n",
      "173  u girls playing great scott allston tonight e ...  \n",
      "174  olin alums spot available bedroom house sublet...  \n",
      "175  hello e planning panel people talk experiences...  \n",
      "176  happening library cone talk cool alumni obtene...  \n",
      "177  want come watch red sox beat yankees fenway le...  \n",
      "178  going hmart run backburner either later today ...  \n",
      "180  jansport backpack sitting window rd floor mila...  \n",
      "181  hey everyone reminder please come talk student...  \n",
      "182  hey carpe another p project another survey tim...  \n",
      "\n",
      "[177 rows x 6 columns]\n",
      "                                       who  \\\n",
      "0                         Grace Montagnino   \n",
      "1                           Jared Briskman   \n",
      "2                       Victoria McDermott   \n",
      "3                              Shane Kelly   \n",
      "4                              Shane Kelly   \n",
      "5                             Samuel Myers   \n",
      "6                         Nicholas Sherman   \n",
      "7                               Sean Foley   \n",
      "8                               Sean Foley   \n",
      "9                      Siddharth Garimella   \n",
      "10                           Rowan Sharman   \n",
      "11                             Miriam Kome   \n",
      "12                           Kevin Crispie   \n",
      "13                            Emily Lepert   \n",
      "14                             Shruti Iyer   \n",
      "15                             Jeremy Ryan   \n",
      "16                             Chloe Grubb   \n",
      "17                        Anupama Krishnan   \n",
      "18                             Shruti Iyer   \n",
      "19                          Rebecca Jordan   \n",
      "20                         Taylor Sheneman   \n",
      "21                         Shawn Albertson   \n",
      "22                         William Manidis   \n",
      "23                      Brennan VandenHoek   \n",
      "24                             Jeremy Ryan   \n",
      "25                             Erika Serna   \n",
      "26                      Brennan VandenHoek   \n",
      "27                        Miranda McMillen   \n",
      "28                         Melissa Anthony   \n",
      "29                         Shawn Albertson   \n",
      "..                                     ...   \n",
      "49             Beverly Walker (Forwarding)   \n",
      "50                         Margaret Rosner   \n",
      "51                          Nina Tchirkova   \n",
      "52                         Alexander Hoppe   \n",
      "53                         Emily Petersell   \n",
      "54                       Lakhvinder Jordan   \n",
      "55                                Emma Pan   \n",
      "56                           Maxmilian Wei   \n",
      "57                              Bryce Mann   \n",
      "58                            William Wong   \n",
      "59                           Maxmilian Wei   \n",
      "60                            Anika Payano   \n",
      "61                             Erika Serna   \n",
      "62                        Gabrielle Clarke   \n",
      "63                       Zachary Davenport   \n",
      "64                       Zachary Davenport   \n",
      "65                            Emily Kohler   \n",
      "66                         Alexander Scott   \n",
      "67  CarpediemOn Behalf OfZachary Davenport   \n",
      "68  CarpediemOn Behalf OfZachary Davenport   \n",
      "69                           Miriam Stulin   \n",
      "70                           Diego Alvarez   \n",
      "71                             Miriam Kome   \n",
      "72                             Chloe Grubb   \n",
      "73                          Vienna Scheyer   \n",
      "74                             Linnea Laux   \n",
      "75                               Arpan Rau   \n",
      "76                         Margaret Rosner   \n",
      "77                           Raquel Dunoff   \n",
      "78                          Jared Briskman   \n",
      "\n",
      "                                                 date  \\\n",
      "0                  Thursday, May 10, 2018 12:48:36 PM   \n",
      "1                  Thursday, May 10, 2018 12:51:53 PM   \n",
      "2                   Thursday, May 10, 2018 1:56:48 PM   \n",
      "3                   Thursday, May 10, 2018 2:58:00 PM   \n",
      "4                   Thursday, May 10, 2018 3:01:28 PM   \n",
      "5                   Thursday, May 10, 2018 4:17:14 PM   \n",
      "6                   Thursday, May 10, 2018 4:30:28 PM   \n",
      "7                   Thursday, May 10, 2018 5:25:20 PM   \n",
      "8                   Thursday, May 10, 2018 6:12:02 PM   \n",
      "9                    Friday, May 11, 2018 12:42:24 AM   \n",
      "10                    Friday, May 11, 2018 3:32:37 AM   \n",
      "11                    Friday, May 11, 2018 8:57:15 AM   \n",
      "12                   Friday, May 11, 2018 10:45:29 AM   \n",
      "13                   Friday, May 11, 2018 10:53:33 AM   \n",
      "14                   Friday, May 11, 2018 11:43:50 AM   \n",
      "15                    Friday, May 11, 2018 4:32:02 PM   \n",
      "16                    Friday, May 11, 2018 5:11:54 PM   \n",
      "17                    Friday, May 11, 2018 6:49:34 PM   \n",
      "18                    Friday, May 11, 2018 7:20:32 PM   \n",
      "19                    Friday, May 11, 2018 8:28:20 PM   \n",
      "20                    Friday, May 11, 2018 9:00:36 PM   \n",
      "21                  Saturday, May 12, 2018 1:21:48 AM   \n",
      "22                  Saturday, May 12, 2018 1:22:29 AM   \n",
      "23                  Saturday, May 12, 2018 1:16:35 PM   \n",
      "24                  Saturday, May 12, 2018 1:39:09 PM   \n",
      "25                  Saturday, May 12, 2018 2:52:50 PM   \n",
      "26                  Saturday, May 12, 2018 3:33:00 PM   \n",
      "27                  Saturday, May 12, 2018 3:36:23 PM   \n",
      "28                  Saturday, May 12, 2018 3:48:35 PM   \n",
      "29                  Saturday, May 12, 2018 3:53:32 PM   \n",
      "..                                                ...   \n",
      "49              Wednesday, April 11, 2018 12:06:51 PM   \n",
      "50              Wednesday, April 11, 2018 10:53:08 AM   \n",
      "51              Wednesday, April 11, 2018 10:23:04 AM   \n",
      "52               Wednesday, April 11, 2018 8:07:04 AM   \n",
      "53                 Tuesday, April 10, 2018 2:02:23 PM   \n",
      "54                Tuesday, April 10, 2018 11:13:23 AM   \n",
      "55                 Thursday, April 5, 2018 3:08:00 PM   \n",
      "56                  Sunday, April 8, 2018 12:33:41 AM   \n",
      "57                 Tuesday, April 3, 2018 10:37:55 PM   \n",
      "58               Wednesday, April 4, 2018 12:58:48 AM   \n",
      "59                  Friday, March 30, 2018 9:01:28 AM   \n",
      "60                 Friday, March 30, 2018 10:43:14 PM   \n",
      "61               Saturday, March 31, 2018 11:34:20 PM   \n",
      "62                   Sunday, April 1, 2018 5:15:16 PM   \n",
      "63                   Friday, April 6, 2018 7:23:04 PM   \n",
      "64                   Friday, April 6, 2018 7:23:04 PM   \n",
      "65               Saturday, March 31, 2018 12:54:35 PM   \n",
      "66                  Monday, April 2, 2018 12:02:48 AM   \n",
      "67  Friday, March 30, 2018 5:00:59 PM (UTC-05:00) ...   \n",
      "68  Friday, March 30, 2018 5:00:59 PM (UTC-05:00) ...   \n",
      "69                   Monday, April 9, 2018 3:40:04 PM   \n",
      "70                  Monday, April 9, 2018 10:02:48 PM   \n",
      "71                   Monday, April 9, 2018 4:32:23 PM   \n",
      "72                   Sunday, April 8, 2018 6:44:15 PM   \n",
      "73                  Tuesday, April 3, 2018 7:03:20 PM   \n",
      "74                  Monday, April 9, 2018 12:53:28 PM   \n",
      "75                  Friday, March 30, 2018 5:47:32 PM   \n",
      "76                  Friday, April 6, 2018 10:56:09 AM   \n",
      "77                   Sunday, April 1, 2018 5:33:20 PM   \n",
      "78                Wednesday, April 4, 2018 4:26:23 PM   \n",
      "\n",
      "                                              subject  \\\n",
      "0   [Carpediem] Donate to the Needham Community Co...   \n",
      "1                      [Carpediem] ORO Show Tomorrow!   \n",
      "2                           [Carpediem] MitD Tomorrow   \n",
      "3   [Carpediem] Free Discrete Textbook and Student...   \n",
      "4   [Carpediem] Resolved: Free Discrete Textbook a...   \n",
      "5             [Carpediem] Magic: the Gathering Draft!   \n",
      "6   Re: [Carpediem] SOLVED You has Discrete? I has...   \n",
      "7                      [Carpediem] Coconut thai order   \n",
      "8            [Carpediem] Resolved: Coconut thai order   \n",
      "9                   [Carpediem] Discrete for cash EOM   \n",
      "10                   [Carpediem] Discrete for Nothing   \n",
      "11  [Carpediem] Selling backpacking pack! +the res...   \n",
      "12  Re: [Carpediem] Quality Suite/Dorm Room Tables...   \n",
      "13                               [Carpediem] Fly Olin   \n",
      "14                                     Zhengyang Feng   \n",
      "15            [Carpediem] ORO in 30 minutes! Library!   \n",
      "16                 Re: [Carpediem] ORO! Now! Library!   \n",
      "17             [Carpediem] Free food by the firepit!!   \n",
      "18                                     Zhengyang Feng   \n",
      "19          [Carpediem] Watch City Steampunk Festival   \n",
      "20                [Carpediem] Fireside, the reckoning   \n",
      "21     [Carpediem] dance floor in the library NOW EOM   \n",
      "22                           carpediem@lists.olin.edu   \n",
      "23                  [Carpediem] Chacos for sale - $20   \n",
      "24                    [Carpediem] Three second videos   \n",
      "25                                open@lists.olin.edu   \n",
      "26          [Carpediem] Brand new vans for sale - $20   \n",
      "27                      [Carpediem] Blue Bins Wanted:   \n",
      "28                     [Carpediem] Giant Hard Wood ;)   \n",
      "29                           carpediem@lists.olin.edu   \n",
      "..                                                ...   \n",
      "49                           Carpediem@lists.olin.edu   \n",
      "50                        [Carpediem] ACRONYM 12-2:30   \n",
      "51                     [Carpediem] SLACfest eve SLAC!   \n",
      "52                    Re: [Carpediem] Third Wave 4/11   \n",
      "53                 [Carpediem] FIRST projects needed!   \n",
      "54  [Carpediem] Mathletes and Athletes, WH4E, Satu...   \n",
      "55                                 [Carpediem] CHEESE   \n",
      "56                    [Carpediem] PAX East on Sunday!   \n",
      "57                            [Carpediem] You hungry?   \n",
      "58  [Carpediem] OPIUM Presents: We hate the Snapch...   \n",
      "59                          [Carpediem] Anime Boston!   \n",
      "60               [Carpediem] Streaming SU in Nord EOM   \n",
      "61                    Rame Hanna; open@lists.olin.edu   \n",
      "62                [Carpediem] Free Monster Energy Tea   \n",
      "63  [Carpediem] MOVIES: 4North and Chill 8-10:30 (...   \n",
      "64  [Carpediem] MOVIES: 4North and Chill 8-10:30 (...   \n",
      "65                      [Carpediem] Babson gym today?   \n",
      "66                [Carpediem] BirthDanyel cake in EH1   \n",
      "67                           carpediem@lists.olin.edu   \n",
      "68                           carpediem@lists.olin.edu   \n",
      "69  [Carpediem] Headshots w/Miriam =E2=80=93 Libra...   \n",
      "70                   [Carpediem] Backburner (Crepes!)   \n",
      "71        [Carpediem] Home Depot Adventure (join me?)   \n",
      "72          [Carpediem] Do you love claymation films?   \n",
      "73  [Carpediem] Choosing Passions vs. Success in C...   \n",
      "74                   [Carpediem] Wellesley ES Classes   \n",
      "75  [Carpediem] Chaat Bazaar (Vada Pav) at Backbur...   \n",
      "76                    [Carpediem] Seder at 6pm in EH1   \n",
      "77                  [Carpediem] FWOP Broadway Karaoke   \n",
      "78  [Carpediem] Come see Oliners present at the Ba...   \n",
      "\n",
      "                                                 body  \\\n",
      "0   From: CarpeSERV [mailto:carpeserv-bounces@list...   \n",
      "1   tl;dr Olin Rock Orchestra is performing tomorr...   \n",
      "2   TLDR: signups for tomorrow here: https://docs....   \n",
      "3   Let me know if you want them and I can set the...   \n",
      "4   On Thu, May 10, 2018 at 2:57 PM Shane Kelly <s...   \n",
      "5   TL;DR: Sign up with your availability here<htt...   \n",
      "6   From: Helpme <helpme-bounces@lists.olin.edu> O...   \n",
      "7   I'm going to order some Coconut Thai in about ...   \n",
      "8   Get Outlook for Android<https://aka.ms/ghei36>...   \n",
      "9   ______________________________________________...   \n",
      "10  I=92m not taking discrete, but if you give me ...   \n",
      "11  I would like to sell my REI Flash 52 backpack ...   \n",
      "12  The tall table is spoken for, the short table ...   \n",
      "13  Here=92s a spreadsheet to figure out how to ge...   \n",
      "14  Hi friends, Another semester is coming to an e...   \n",
      "15  We're Olin! We're sort of rock! And we're almo...   \n",
      "16  They are awesome, as always. With ORO love, Ch...   \n",
      "17  Come eat and talk to us about Student Governme...   \n",
      "18  s! East Hall 3 Lounge. Bring cups for drinks. ...   \n",
      "19  The Splendiferous Sixpenny Steampunk Society=9...   \n",
      "20  Are you excited finals are over?! Are you just...   \n",
      "21  <3 ___________________________________________...   \n",
      "22  Leon DJ-ing > On May 12, 2018, at 1:21 AM, Sha...   \n",
      "23  I have some men=92s size 11 black chacos that ...   \n",
      "24  Compiling several as an entry to the 30-second...   \n",
      "25  Good evening, I have chips and nacho cheese be...   \n",
      "26  I have navy blue vans that I have litterally w...   \n",
      "27  I need ONE More blue bin. If you have an xtra ...   \n",
      "28  [image1.jpeg] [image2.jpeg][image3.png] Nice a...   \n",
      "29  Brennan is selling his van for $20 On May 12, ...   \n",
      "..                                                ...   \n",
      "49  =93 Sign up for your 10-minute slot today Happ...   \n",
      "50  We Are Back! Thanks to everyone for the warm g...   \n",
      "51  [image: image003.png@01D33205]SLAC 4/11/18 =E2...   \n",
      "52  Open now! Eom On Tue, Apr 10, 2018 at 11:35 PM...   \n",
      "53  Hello students, Susan Brisson and I will each ...   \n",
      "54  Your last chance at a Varsity Letter. Mathlete...   \n",
      "55  Offering cheese and blueberries in the library...   \n",
      "56  Hi everyone,                I=E2=80=99m headed...   \n",
      "57  Hi Carpe, Our P&M team is testing French toast...   \n",
      "58  THURSDAY 10:30PM THE NORD [image: Snapchat-732...   \n",
      "59  I=E2=80=99m headed to Anime Boston at 1 PM tod...   \n",
      "60  Anika Payano Sent from my iPhone _____________...   \n",
      "61  Hello, TL;DR TDoV is March 31. Links for more ...   \n",
      "62  Our suite somehow came by a couple cans of Mon...   \n",
      "63  Hi there, 4N Film Fanatics, We sent out a surv...   \n",
      "64  Hi there, 4N Film Fanatics, We sent out a surv...   \n",
      "65  Sometime before dinner maybe? Message me if yo...   \n",
      "66  It is Danny's BirthDanyel and we have extra Bi...   \n",
      "67  [Carpediem] Want data to make strange correlat...   \n",
      "68  [Carpediem] Want data to make strange correlat...   \n",
      "69  10-minute slot today Dear Oliners, I will be t...   \n",
      "70  Hello again! Backburner will be open this Frid...   \n",
      "71  I'm going on an adventure to home depot today....   \n",
      "72  Then come watch Isle of Dogs with me! I really...   \n",
      "73  Tl;dr: Discussion - This Saturday at 6pm in th...   \n",
      "74  Hello, If you=E2=80=99re interested in maybe t...   \n",
      "75  Dearest Carpe, I know that you've suffered, th...   \n",
      "76  JOO will be hosting a Passover Seder in EH1 at...   \n",
      "77  FWOP and SAC presents Broadway Karaoke! April ...   \n",
      "78  4 Olin teams are presenting at the Babson Vent...   \n",
      "\n",
      "                                       processed_body  \n",
      "0   carpeserv mailto carpeserv bounces lists olin ...  \n",
      "1   tl dr olin rock orchestra performing tomorrow ...  \n",
      "2   tldr signups tomorrow https docs google com sp...  \n",
      "3               let know want set outside suite shane  \n",
      "4   thu may pm shane kelly shane kelly students ol...  \n",
      "5   tl dr sign availability https www meet com pat...  \n",
      "6   helpme helpme bounces lists olin edu behalf ni...  \n",
      "7   going order coconut thai minutes want l et kno...  \n",
      "8   get outlook android https aka ms ghei thu may ...  \n",
      "9   carpediem mailing list carpediem lists olin ed...  \n",
      "10  taking discrete give textbook sell someone cas...  \n",
      "11  would like sell rei flash backpack picture htt...  \n",
      "12  tall table spoken short table footprint still ...  \n",
      "13  spreadsheet figure get airport next week https...  \n",
      "14  hi friends another semester coming end exchang...  \n",
      "15                    olin sort rock almost orchestra  \n",
      "16  awesome always oro love chloe sent iphone may ...  \n",
      "17            come eat talk us student government ana  \n",
      "18  east hall lounge bring cups drinks carpediem c...  \n",
      "19  splendiferous sixpenny steampunk society tradi...  \n",
      "20  excited finals kind tired dead yeah though wel...  \n",
      "21  carpediem mailing list carpediem lists olin ed...  \n",
      "22  leon dj ing may shawn albertson shawn albertso...  \n",
      "23  men size black chacos need let know f want loo...  \n",
      "24  compiling several entry second video send thre...  \n",
      "25  good evening chips nacho cheese miss home also...  \n",
      "26  navy blue vans litterally worn size l et know ...  \n",
      "27  need one blue bin xtra one willing sell e lemm...  \n",
      "28  image jpeg image jpeg image png nice green tri...  \n",
      "29  brennan selling van may pm brennan vandenhoek ...  \n",
      "..                                                ...  \n",
      "49  sign minute slot today happening doc lab tue a...  \n",
      "50  back thanks everyone warm get well wishes tric...  \n",
      "51  image image png slac e need one slac got slac ...  \n",
      "52  open eom tue apr pm alexander hoppe forwarding...  \n",
      "53  hello students susan brisson attending one fir...  \n",
      "54  last chance varsity letter mathletes athletes ...  \n",
      "55  offering cheese blueberries library thoughts t...  \n",
      "56  hi everyone e headed pax east sunday cover cou...  \n",
      "57  hi carpe p team testing french toast recipes w...  \n",
      "58  thursday pm nord image snapchat jpg fb event h...  \n",
      "59  e headed anime boston pm today anyone wants sh...  \n",
      "60  anika payano sent iphone carpediem mailing lis...  \n",
      "61  hello tl dr tdov march links tdov list trans p...  \n",
      "62  suite somehow came couple cans monster e wa nt...  \n",
      "63  hi n film fanatics sent survey earlier top cho...  \n",
      "64  hi n film fanatics sent survey earlier top cho...  \n",
      "65    sometime dinner maybe message want join j emily  \n",
      "66  danny birthdanyel extra birthdanyel cake come ...  \n",
      "67  carpediem want data make strange correlations ...  \n",
      "68  carpediem want data make strange correlations ...  \n",
      "69  minute slot today dear oliners taking headshot...  \n",
      "70  hello backburner open friday usual time place ...  \n",
      "71  going adventure home depot today timing flexib...  \n",
      "72  come watch isle dogs really want see would lov...  \n",
      "73  tl dr discussion saturday pm crescent room sna...  \n",
      "74  hello e interested maybe taking wellesley envi...  \n",
      "75  dearest carpe know suffered past months trying...  \n",
      "76  joo hosting passover seder eh pm made effort m...  \n",
      "77  fwop sac presents broadway karaoke april th pm...  \n",
      "78  olin teams presenting babson venture expo till...  \n",
      "\n",
      "[77 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# get train & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"./data/train.csv\", sep=',')\n",
    "test = pd.read_csv(\"./data/test.csv\", sep=',')\n",
    "#clean data\n",
    "train = train.dropna()\n",
    "#warning dropna can drop a lot of data if you have empty columns in the csv file\n",
    "test = test.dropna()\n",
    "train['processed_body'] = train['body'].apply(text_to_words)\n",
    "test['processed_body'] = test['body'].apply(text_to_words)\n",
    "\n",
    "#print and check the data is what we think it is.\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cleaned up data we can proceed with our model - bag of words: which will create a bag of words using the words from all of our entries and counts them. We will display the 1000 most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 1000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(train['processed_body'].tolist())\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 10) \n",
    "\n",
    "# Fit the forest to the training data,using the bag of words as \n",
    "#features and the tag labels as response variable\n",
    "\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[\"Tag\"] )\n",
    "\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array# Get a \n",
    "test_data_features = vectorizer.transform(test['processed_body'].tolist())\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make tag label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "def submission(predictions):\n",
    "    \"\"\"\n",
    "    Use pandas to write the comma-separated output \n",
    "    file based on the predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "            \"Review\": test[\"who\"],\n",
    "            \"Tag\": predictions\n",
    "        })\n",
    "\n",
    "    submission.to_csv(\"Test_Results.csv\", index=False, quoting=3)\n",
    "\n",
    "#Use pandas to write the comma-separated output file\n",
    "#prediction_generator(result, '/data/predictions.csv')\n",
    "\n",
    "\n",
    "submission(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To do Naive Bayesian we will need to represent each of the 28 possible tags with a class probability to represent each potential outcome. We will get the frequency per word and then calculate the probability for each of the class probabilites by dividing the frequency of word with the total number of occurences that word occurs in that body of the email. To get the probability we multiply the probability for each word in that body against the probability of any document that expresses that tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start by creating 29 probabilities for any given document that expresses that tag\n",
    "doc_prob_0 = float(len(train[train.Tag == 0]))/len(train)\n",
    "doc_prob_1 = float(len(train[train.Tag == 1]))/len(train)\n",
    "doc_prob_2 = float(len(train[train.Tag == 2]))/len(train)\n",
    "doc_prob_3 = float(len(train[train.Tag == 3]))/len(train)\n",
    "doc_prob_4 = float(len(train[train.Tag == 4]))/len(train)\n",
    "doc_prob_5 = float(len(train[train.Tag == 5]))/len(train)\n",
    "doc_prob_6 = float(len(train[train.Tag == 6]))/len(train)\n",
    "doc_prob_7 = float(len(train[train.Tag == 7]))/len(train)\n",
    "doc_prob_8 = float(len(train[train.Tag == 8]))/len(train)\n",
    "doc_prob_9 = float(len(train[train.Tag == 9]))/len(train)\n",
    "doc_prob_10 = float(len(train[train.Tag == 10]))/len(train)\n",
    "doc_prob_11 = float(len(train[train.Tag == 11]))/len(train)\n",
    "doc_prob_12 = float(len(train[train.Tag == 12]))/len(train)\n",
    "doc_prob_13 = float(len(train[train.Tag == 13]))/len(train)\n",
    "doc_prob_14 = float(len(train[train.Tag == 14]))/len(train)\n",
    "doc_prob_15 = float(len(train[train.Tag == 15]))/len(train)\n",
    "doc_prob_16 = float(len(train[train.Tag == 16]))/len(train)\n",
    "doc_prob_17 = float(len(train[train.Tag == 17]))/len(train)\n",
    "doc_prob_18 = float(len(train[train.Tag == 18]))/len(train)\n",
    "doc_prob_19 = float(len(train[train.Tag == 19]))/len(train)\n",
    "doc_prob_20 = float(len(train[train.Tag == 20]))/len(train)\n",
    "doc_prob_21 = float(len(train[train.Tag == 21]))/len(train)\n",
    "doc_prob_22 = float(len(train[train.Tag == 22]))/len(train)\n",
    "doc_prob_23 = float(len(train[train.Tag == 23]))/len(train)\n",
    "doc_prob_24 = float(len(train[train.Tag == 24]))/len(train)\n",
    "doc_prob_25 = float(len(train[train.Tag == 25]))/len(train)\n",
    "doc_prob_26 = float(len(train[train.Tag == 26]))/len(train)\n",
    "doc_prob_27 = float(len(train[train.Tag == 27]))/len(train)\n",
    "doc_prob_28 = float(len(train[train.Tag == 28]))/len(train)\n",
    "\n",
    "doc_probs = [doc_prob_0,doc_prob_1, doc_prob_2, doc_prob_3, doc_prob_4, doc_prob_5, doc_prob_6, doc_prob_7, doc_prob_8, doc_prob_9, doc_prob_10, doc_prob_11, doc_prob_12, doc_prob_13, doc_prob_14, doc_prob_15, doc_prob_16, doc_prob_17, doc_prob_18, doc_prob_19, doc_prob_20, doc_prob_21, doc_prob_22, doc_prob_23, doc_prob_24, doc_prob_25, doc_prob_26, doc_prob_27, doc_prob_28]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions to run our Naive Bayesian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_word(df):\n",
    "    \"\"\" Takes: Test data in the form of dataframe\n",
    "        Returns: A list value of words number in different tag\n",
    "    \"\"\"\n",
    "    tag_word = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    for index, row in df.iterrows():\n",
    "        tag = row[\"Tag\"]\n",
    "        tag_word[tag] += (row[\"processed_body\"].count(\" \") + 1)\n",
    "    return tag_word\n",
    "\n",
    "def create_reference_histogram(df):\n",
    "    \"\"\"Takes: A dataset of strings\n",
    "       Returns: Dictionary w/ keys-words, value- list of values[sent=0,sent=1,sent=2,sent=3,sent=4,total] \n",
    "    \"\"\"\n",
    "    histogram = {}\n",
    "    #we will iterate through each sentence and word and increment the index of the list of values\n",
    "    for index, row in df.iterrows():\n",
    "        tag_value = df.Tag[index]\n",
    "        words = df[\"processed_body\"][index].split()\n",
    "        for word in words:\n",
    "            if word in histogram:\n",
    "                histogram[word][5] +=1\n",
    "                histogram[word][tag_value] += 1\n",
    "            else:\n",
    "                histogram[word]= np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "                histogram[word][tag_value] += 1\n",
    "    return histogram  \n",
    "\n",
    "def calc_tag_probabilities(ref_dict,tag_freq):\n",
    "    \"\"\" Takes: A tag value you are trying to test for\n",
    "        Returns: Dictionary w/ keys-word value-probability of that word matching the value\n",
    "        frequency = matching_value_freq/total_freq\n",
    "    \"\"\"   \n",
    "    # make a copy of ref_dict\n",
    "    final_dict = {}\n",
    "    # we will iterate through every word in the dictionary and change the values from a list to the probability we want \n",
    "    # if the value is 0 we will add 1 as part of smooth\n",
    "    for word in ref_dict:\n",
    "        final_dict[word] = (ref_dict[word][:-1]+1)/(tag_freq + float(ref_dict[word][28]))\n",
    "    return final_dict\n",
    "\n",
    "def smooth(lst):\n",
    "    \"\"\"\n",
    "    Intended to ignore new words by making probability 1\n",
    "    \"\"\"\n",
    "    for ele in lst:\n",
    "        if ele == 0: ele = 1\n",
    "    return lst\n",
    "\n",
    "def phrase_tag_prob(text, refer):\n",
    "    \"\"\" Takes: A phrase in the form of a string\n",
    "        Returns: A list of probabilities for different sentiments\n",
    "    \"\"\"\n",
    "    probs = np.asarray(doc_probs)\n",
    "    for word in text.split():\n",
    "        if word in refer:\n",
    "            probs *= smooth(refer[word])\n",
    "    return probs    \n",
    "\n",
    "def make_predictions(data, refer):\n",
    "    \"\"\" Takes: Test data in the form of dataframe\n",
    "        Returns: Submission File w. Sentiment value calculated\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    lst = []\n",
    "    for index, row in data.iterrows():\n",
    "        lst = phrase_tag_prob(data['processed_body'][index], refer).tolist()\n",
    "        result.append(lst.index(max(lst)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the following functions in order and generate our prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wilsontang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/wilsontang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:57: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "overall_tag = tag_word(train)\n",
    "hist = create_reference_histogram(train)\n",
    "reference_hist = calc_tag_probabilities(hist,overall_tag)\n",
    "result = make_predictions(test,reference_hist)\n",
    "submission(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
